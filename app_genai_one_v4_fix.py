import streamlit as st
import os
import json
import time
import re
import wave
import difflib
import contextlib
import io
import base64
from ebooklib import epub, ITEM_DOCUMENT
from bs4 import BeautifulSoup
from google import genai
from google.genai import types
from pydub import AudioSegment

# ==========================================
# [0] ê¸°ë³¸ ì„¤ì • ë° ìƒìˆ˜
# ==========================================
SCRIPT_ROOT = "playground_scripts"
AUDIO_ROOT = "playground_audio"
REMAINING_FILE = "remaining_source.txt"

# ëª¨ë¸ ì„¤ì •
TTS_MODEL_ID = "gemini-2.5-pro-preview-tts" 
VERIFY_MODEL_ID = "gemini-2.5-pro"
ANALYZE_MODEL_ID = "gemini-3-pro-preview"

# ë³´ì´ìŠ¤ ëª©ë¡
VOICE_OPTIONS = ["Puck", "Charon", "Kore", "Fenrir", "Aoede", "Zephyr"]

os.makedirs(SCRIPT_ROOT, exist_ok=True)
os.makedirs(AUDIO_ROOT, exist_ok=True)

# ==========================================
# [1] ìœ í‹¸ë¦¬í‹°: í…ìŠ¤íŠ¸ ì²˜ë¦¬
# ==========================================
def save_text_file(path, content):
    with open(path, 'w', encoding='utf-8') as f:
        f.write(content)

def load_text_file(path):
    if os.path.exists(path):
        with open(path, 'r', encoding='utf-8') as f:
            return f.read()
    return ""

def get_full_text_from_epub(file):
    """
    EPUBì˜ Spine(ì½ëŠ” ìˆœì„œ)ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ìˆœì„œëŒ€ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    with open("temp.epub", "wb") as f:
        f.write(file.getbuffer())
        
    full_text = []
    
    try:
        book = epub.read_epub("temp.epub")
        
        # [í•µì‹¬ ìˆ˜ì •] get_items() ëŒ€ì‹  spineì„ ìˆœíšŒí•©ë‹ˆë‹¤.
        # book.spineì€ (item_id, linear) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
        for item_id, _ in book.spine:
            item = book.get_item_with_id(item_id)
            
            # ì•„ì´í…œì´ ì¡´ì¬í•˜ê³ , ë¬¸ì„œ(HTML) íƒ€ì…ì¸ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
            if item and item.get_type() == ITEM_DOCUMENT:
                soup = BeautifulSoup(item.get_content(), 'html.parser')
                
                # [ì¶”ê°€ íŒ] Scriptë‚˜ Style íƒœê·¸ëŠ” í…ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë¯€ë¡œ ì œê±° (ë” ê¹”ë”í•´ì§)
                for script in soup(["script", "style"]):
                    script.extract()
                    
                text = soup.get_text(separator='\n').strip()
                
                if text:
                    full_text.append(text)
                    
        return "\n\n".join(full_text)
        
    except Exception as e:
        print(f"Epub Extract Error: {e}")
        return ""
        
    finally:
        if os.path.exists("temp.epub"):
            try:
                os.remove("temp.epub")
            except:
                pass

def sanitize_filename(name):
    name = re.sub(r'[\\/*?:"<>|]', "", name)
    name = name.replace(" ", "_")
    return name.strip()[:50]

def get_subdirectories(root_path):
    if not os.path.exists(root_path): return []
    return sorted([d for d in os.listdir(root_path) if os.path.isdir(os.path.join(root_path, d))])

def get_files_in_dir(dir_path, ext):
    if not os.path.exists(dir_path): return []
    return sorted([f for f in os.listdir(dir_path) if f.endswith(ext)])

def process_text_for_playground(text, max_chars=1600):
    """
    í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬:
    1. ë¶ˆí•„ìš”í•œ ê´„í˜¸ ë° ë‚´ìš© ì‚­ì œ (ì£¼ì„, ì§€ë¬¸ ë“±)
    2. ê³µë°± ì •ë¦¬
    3. ë¬¸ì¥ ë‹¨ìœ„ë¡œ ëŠì–´ì„œ max_chars ê¸¸ì´ì— ë§ê²Œ ì²­í¬ ë¶„í• 
    """
    # [NEW] 1. ê´„í˜¸ì™€ ê·¸ ì•ˆì˜ ë‚´ìš© í†µì§¸ë¡œ ì‚­ì œ
    # ì˜ˆ: "ì•ˆë…•í•˜ì„¸ìš”(ì›ƒìŒ)" -> "ì•ˆë…•í•˜ì„¸ìš”"
    text = re.sub(r'\([^)]*\)', '', text)   # (ì†Œê´„í˜¸) ì œê±°
    text = re.sub(r'\[[^\]]*\]', '', text)  # [ëŒ€ê´„í˜¸] ì œê±°
    text = re.sub(r'\{[^}]*\}', '', text)   # {ì¤‘ê´„í˜¸} ì œê±°
    text = re.sub(r'\<[^>]*\>', '', text)   # <êº½ì‡ > ì œê±°
    
    # 2. íŠ¹ìˆ˜ë¬¸ì ë° ê³µë°± ì •ë¦¬
    # ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ ë³€ê²½
    text = text.replace("\n", " ")
    # ë§ì¤„ì„í‘œ ì •ê·œí™”
    text = text.replace("...", ",").replace("â€¦", ",")
    # ë‚¨ì€ íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•„ìš”í•˜ë‹¤ë©´ * # @ ë“±)
    text = re.sub(r'[\*\#\@]', '', text)
    
    # ë‹¤ì¤‘ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì¤„ì„ ("  " -> " ")
    text = re.sub(r'\s+', ' ', text).strip()
    
    # 3. ì²­í¬ ë¶„í•  (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    chunks = []
    current_chunk = ""
    # ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ ë’¤ì—ì„œ ë¬¸ì¥ ë¶„ë¦¬
    sentences = re.split(r'(?<=[.?!])\s+', text)
    
    for sentence in sentences:
        if not sentence: continue
        
        # ì²­í¬ í¬ê¸° ì œí•œ í™•ì¸
        if len(current_chunk) + len(sentence) > max_chars:
            if current_chunk: chunks.append(current_chunk.strip())
            current_chunk = sentence
        else:
            if current_chunk: current_chunk += " " + sentence
            else: current_chunk = sentence
                
    if current_chunk: chunks.append(current_chunk.strip())
    return chunks


# ==========================================
# [1-2] ìœ í‹¸ë¦¬í‹°: íƒ€ì„ë¼ì¸ ìë™ ë³´ì • (ì‚­ì œ ì‹œ)
# ==========================================
def adjust_timeline_for_deletion(timeline, del_start, del_end):
    gap = del_end - del_start
    new_timeline = []
    
    for item in timeline:
        # 1. ì‚­ì œ êµ¬ê°„ë³´ë‹¤ ì•ì— ìˆëŠ” ë¬¸ì¥ -> ì˜í–¥ ì—†ìŒ
        if item['end'] <= del_start:
            new_timeline.append(item)
            
        # 2. ì‚­ì œ êµ¬ê°„ë³´ë‹¤ ë’¤ì— ìˆëŠ” ë¬¸ì¥ -> ì‹œê°„ ë‹¹ê¹€ (Gapë§Œí¼ ë¹¼ê¸°)
        elif item['start'] >= del_end - 0.05: # 0.05ëŠ” ë¯¸ì„¸ ì˜¤ì°¨ í—ˆìš©
            item['start'] = max(0, item['start'] - gap)
            item['end'] = max(0, item['end'] - gap)
            new_timeline.append(item)
            
        # 3. ì‚­ì œ êµ¬ê°„ì— ê±¸ì³ ìˆëŠ” ë¬¸ì¥ -> ê¸¸ì´ ì¤„ì„
        else:
            # ë’·ë¶€ë¶„ì´ ì˜ë ¤ë‚˜ê°”ìœ¼ë¯€ë¡œ ëë‚˜ëŠ” ì‹œê°„ë§Œ ë‹¹ê¹€
            # (ë‹¨, ì‹œì‘ ì‹œê°„ì€ ê·¸ëŒ€ë¡œ)
            item['end'] = max(item['start'], item['end'] - gap)
            new_timeline.append(item)
            
    return new_timeline

# ==========================================
# [1-3] ìœ í‹¸ë¦¬í‹°: íƒ€ì„ë¼ì¸ ìë™ ë³´ì • (ì¬ë…¹ìŒ/íŒ¨ì¹­ ì‹œ)
# ==========================================
def adjust_timeline_for_patch(timeline, target_start, target_end, new_duration):
    """
    target_start ~ target_end êµ¬ê°„ì´ new_duration ê¸¸ì´ë¡œ ë°”ë€Œì—ˆì„ ë•Œ,
    ë’·ë¶€ë¶„ì„ ë°€ê±°ë‚˜ ë‹¹ê¹ë‹ˆë‹¤.
    """
    old_duration = target_end - target_start
    diff = new_duration - old_duration # ì–‘ìˆ˜ë©´ ë°€ë¦¬ê³ , ìŒìˆ˜ë©´ ë‹¹ê²¨ì§
    
    # ì˜¤ì°¨ ë²”ìœ„ (ì¬ë…¹ìŒëœ êµ¬ê°„ì˜ ë ì§€ì )
    patch_end_point = target_start + new_duration
    
    for item in timeline:
        # 1. ì¬ë…¹ìŒ êµ¬ê°„ ì´í›„ì˜ ë¬¸ì¥ë“¤ -> ì°¨ì´(diff)ë§Œí¼ ì´ë™
        if item['start'] >= target_end - 0.1:
            item['start'] += diff
            item['end'] += diff
            
        # 2. ì¬ë…¹ìŒëœ ë°”ë¡œ ê·¸ ë¬¸ì¥ (ë˜ëŠ” ê·¸ ì•ˆì— í¬í•¨ëœ ë¬¸ì¥)
        elif item['end'] > target_start and item['start'] < target_end:
            # ì´ ë¬¸ì¥ì˜ ëì„ ìƒˆë¡œìš´ ì˜¤ë””ì˜¤ ëì— ë§ì¶¤ (ë‹¨ìˆœí™”)
            # ì—¬ëŸ¬ ë¬¸ì¥ì´ ê²¹ì¹œ ê²½ìš° ë³µì¡í•˜ì§€ë§Œ, ë³´í†µ 1ë¬¸ì¥ ìˆ˜ì„ ì´ë¯€ë¡œ ì´ ë°©ì‹ì´ ìœ íš¨í•¨
            item['end'] += diff 
            
    return timeline

# ==========================================
# [2] ìœ í‹¸ë¦¬í‹°: ì˜¤ë””ì˜¤ ìƒì„± & ì²˜ë¦¬ (Raw PCM ëŒ€ì‘ íŒ¨ì¹˜)
# ==========================================

def add_silence_padding(audio_bytes, duration_sec=0.5):
    """
    [ìµœì¢… ìˆ˜ì •] Raw PCM ëŒ€ì‘ ê°•í™”
    - RIFF í—¤ë”ê°€ ìˆìœ¼ë©´ WAVë¡œ ì¸ì‹
    - í—¤ë”ê°€ ì—†ê³  0000ìœ¼ë¡œ ì‹œì‘í•´ë„, ë°ì´í„°ê°€ ì¶©ë¶„íˆ í¬ë©´ Raw PCMìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ì‚´ë ¤ëƒ„
    """
    # 1. 1ì°¨ ë°©ì–´: ë°ì´í„°ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´(100ë°”ì´íŠ¸ ë¯¸ë§Œ) ì§„ì§œ ì˜¤ë¥˜
    if not audio_bytes or len(audio_bytes) < 100:
        print("Padding Error: ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return None

    # Gemini ê¸°ë³¸ ì˜¤ë””ì˜¤ ìŠ¤í™ (Raw PCMì¼ ê²½ìš° ì ìš©í•  ê°’)
    # ëª¨ë¸ì— ë”°ë¼ 24000Hzê°€ ê¸°ë³¸ì¸ ê²½ìš°ê°€ ë§ìŒ
    CHANNELS = 1
    SAMPWIDTH = 2
    FRAMERATE = 24000
    
    output_io = io.BytesIO()
    
    try:
        raw_data = audio_bytes
        
        # 2. ë°ì´í„° í˜•ì‹ íŒë‹¨ (WAV vs Raw PCM)
        if audio_bytes.startswith(b'RIFF'):
            # WAV íŒŒì¼ì¸ ê²½ìš°: í—¤ë” ì •ë³´ë¥¼ ì½ì–´ì„œ ì„¸íŒ…
            try:
                with wave.open(io.BytesIO(audio_bytes), 'rb') as wav_in:
                    CHANNELS = wav_in.getnchannels()
                    SAMPWIDTH = wav_in.getsampwidth()
                    FRAMERATE = wav_in.getframerate()
                    raw_data = wav_in.readframes(wav_in.getnframes())
            except wave.Error:
                print("Warning: RIFF í—¤ë”ê°€ ìˆì§€ë§Œ wave ëª¨ë“ˆì´ ì½ì§€ ëª»í•¨. Rawë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.")
        else:
            # WAVê°€ ì•„ë‹Œ ê²½ìš° (RIFF ì—†ìŒ):
            # 00 00 00 00ìœ¼ë¡œ ì‹œì‘í•˜ë”ë¼ë„ Raw PCMìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì§„í–‰
            # ë³„ë„ ì²˜ë¦¬ ì—†ì´ raw_data = audio_bytes ê·¸ëŒ€ë¡œ ì‚¬ìš©
            pass

        # 3. ë¬´ìŒ ë°ì´í„° ìƒì„±
        num_silent_frames = int(FRAMERATE * duration_sec)
        silent_data = b'\x00' * (num_silent_frames * CHANNELS * SAMPWIDTH)

        # 4. ë°ì´í„° ê²°í•© (ë¬´ìŒ + ì›ë³¸)
        final_data = silent_data + raw_data

        # 5. ìƒˆë¡œìš´ WAV ì»¨í…Œì´ë„ˆ ì”Œìš°ê¸° (Re-packaging)
        with wave.open(output_io, 'wb') as wav_out:
            wav_out.setnchannels(CHANNELS)
            wav_out.setsampwidth(SAMPWIDTH)
            wav_out.setframerate(FRAMERATE)
            wav_out.writeframes(final_data)
            
        return output_io.getvalue()

    except Exception as e:
        print(f"Padding Critical Error: {e}")
        return None

def generate_speech(api_key, text, prompt, voice_name, temperature, speed_rate=1.0, volume_db=0.0):
    """
    [ìˆ˜ì •ë¨] volume_db íŒŒë¼ë¯¸í„° í¬í•¨ ë²„ì „
    """
    client = genai.Client(api_key=api_key)
    
    # ê°€ì´ë“œë¼ì¸ (ê¸°ì¡´ ë‚´ìš© ìœ ì§€)
    audio_guidelines = """
    [Audio Engineering Guidelines - STRICTLY FOLLOW]:
    1. Tone: ìì—°ìŠ¤ëŸ¬ìš´ ì¼ì‹œ ì •ì§€ë¥¼ í¬í•¨í•œ ë¶€ë“œëŸ½ê³  ì°¨ë¶„í•˜ë©° ë“£ê¸° í¸ì•ˆí•œ ì±… ì½ê¸° í†¤ì„ ìœ ì§€í•©ë‹ˆë‹¤.
    2. Pitch: 0.0ìœ¼ë¡œ ì„¤ì •í•˜ë©°, ì²˜ìŒë¶€í„° ëê¹Œì§€ ì¼ê´€ë˜ê²Œ ìœ ì§€í•©ë‹ˆë‹¤.
    3. Speaking Rate: -1.0 (Slow/Steady)ìœ¼ë¡œ ì„¤ì •í•˜ë©°, ì²˜ìŒë¶€í„° ëê¹Œì§€ ëŠë¦¬ê³  ê¾¸ì¤€í•˜ë©° ê· ì¼í•˜ê²Œ ìœ ì§€í•©ë‹ˆë‹¤.
    4. Volume: ì²˜ìŒë¶€í„° ëê¹Œì§€ ì¼ê´€ë˜ê³  ê· í˜• ìˆê²Œ ìœ ì§€í•©ë‹ˆë‹¤.
    5. Initial Word Intonation: ì²« ë‹¨ì–´ ì‹œì‘ ì‹œ ê°•í•œ ì–µì–‘ì„ í”¼í•˜ê³ , í‰íƒ„í•˜ê³  ì•ˆì •ì ì¸ ì‹œì‘ì„ ìœ ì§€í•©ë‹ˆë‹¤.
    6. Emphasis: íŠ¹ì • ë‹¨ì–´ì— ëŒ€í•œ ê°•ì¡° ì—†ì´ í‰íƒ„í•˜ê³  ê· ì¼í•˜ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤.
    7. Pauses between sentences: ì§§ê³  ìì—°ìŠ¤ëŸ¬ìš´ ì¼ì‹œ ì •ì§€ë§Œ í—ˆìš©í•˜ë©°, ê¸´ ì¼ì‹œ ì •ì§€ë‚˜ ë“¤ë¦¬ëŠ” ìˆ¨ì†Œë¦¬ëŠ” í”¼í•©ë‹ˆë‹¤.
    8. Breathing Sounds / Breath Control: ìˆ¨ì†Œë¦¬ëŠ” ì™„ì „íˆ ì œê±°ë˜ë„ë¡ ìµœì†Œí™”í•˜ë©° ë“¤ë¦¬ì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤. ë¬¸ì¥ ì¤‘ ë˜ëŠ” ë¬¸ì¥ ì‚¬ì´ì—ì„œ ìˆ¨ì†Œë¦¬ê°€ ì—†ì–´ì•¼ í•˜ë©°, ìµœì¢… ë¬¸ì¥ì˜ ëê¹Œì§€ ë¬´ìŒì´ ìœ ì§€ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    9. Sibilance Suppression / Consonant Handling: ã……, ã…†, ã…Š, ã…ˆ, ã… ë“± ë§ˆì°°ìŒì€ ë‚ ì¹´ë¡­ê±°ë‚˜ ê±°ì¹ ê²Œ ë“¤ë¦¬ì§€ ì•Šë„ë¡ ë¶€ë“œëŸ½ê³  ìœ ì—°í•˜ê²Œ ë°œìŒí•©ë‹ˆë‹¤ (ì™„ì „ ì–µì œ).
    10. Consistency: ì²˜ìŒë¶€í„° ë§ˆì§€ë§‰ ë¬¸ì¥ê¹Œì§€, ê·¸ë¦¬ê³  ëª¨ë“  ì¬ìƒì„± ê³¼ì •ì— ê±¸ì³ ë™ì¼í•œ Pitch, Tone, Speed, ë§í•˜ëŠ” ìŠ¤íƒ€ì¼ì„ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤.
    11. Naturalness: ì£¼ì €í•¨, ë”ë“¬ê±°ë¦¼, ì™œê³¡, Robotic artifacts ì—†ì´ ë¶€ë“œëŸ½ê³  ì—°ì†ì ìœ¼ë¡œ ì½ìŠµë‹ˆë‹¤.
    12. Audio quality: ê¹¨ë—í•˜ê³  ëª…í™•í•˜ë©° ë°©ì†¡ í’ˆì§ˆ ìˆ˜ì¤€ì˜ ê²°ê³¼ë¬¼ì„ ì œê³µí•©ë‹ˆë‹¤.
    """

    system_prompt = (
        "You are a professional AI Audio Book Narrator and Mastering Engineer. "
        "Your goal is to generate high-fidelity speech that sounds exactly like a professional broadcast recording."
        "DO NOT generate any text response. JUST generate the AUDIO output."
    )
    
    full_prompt = f"""
    {system_prompt}

    {audio_guidelines}

    [User Style Instruction]:
    {prompt}

    [Speed Control]:
    The base speaking rate is set to '-1.0' (Slow) as per guidelines.
    Current Adjustment: Please apply a speed multiplier of {speed_rate}x to this base rate.

    [Text to Read]:
    {text}
    """
    
    config = types.GenerateContentConfig(
        temperature=temperature,
        response_modalities=["AUDIO"], 
        speech_config=types.SpeechConfig(
            voice_config=types.VoiceConfig(
                prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name=voice_name)
            )
        )
    )

    try:
        # 1. AI ì˜¤ë””ì˜¤ ìƒì„± ìš”ì²­
        response = client.models.generate_content(
            model=TTS_MODEL_ID,
            contents=[types.Content(role="user", parts=[types.Part.from_text(text=full_prompt)])],
            config=config
        )
        
        raw_audio = None
        for part in response.candidates[0].content.parts:
            if part.inline_data and part.inline_data.data:
                raw_audio = part.inline_data.data
                break
        
        # 2. ë³¼ë¥¨ ì¡°ì ˆ ë¡œì§ (pydub ì‚¬ìš©)
        if raw_audio:
            if volume_db != 0.0:
                try:
                    s = AudioSegment.from_wav(io.BytesIO(raw_audio))
                    s = s + volume_db 
                    out = io.BytesIO()
                    s.export(out, format="wav")
                    return out.getvalue()
                except Exception as vol_err:
                    print(f"Volume Adjust Error: {vol_err}")
                    return raw_audio
            else:
                return raw_audio

    except Exception as e:
        print(f"Generate API Error: {e}")
        
    return None

def delete_audio_range(original_wav_bytes, start_sec, end_sec):
    """
    ì˜¤ë””ì˜¤ì˜ íŠ¹ì • êµ¬ê°„(start_sec ~ end_sec)ì„ ë¬¼ë¦¬ì ìœ¼ë¡œ ì‚­ì œí•˜ê³  ì´ì–´ ë¶™ì…ë‹ˆë‹¤.
    ì˜ˆ: 13ì´ˆ~15ì´ˆ êµ¬ê°„ì„ ì‚­ì œí•˜ë©´, 15ì´ˆì— ìˆë˜ ì†Œë¦¬ê°€ 13ì´ˆë¡œ ë‹¹ê²¨ì§‘ë‹ˆë‹¤.
    """
    try:
        if not original_wav_bytes: return None
        
        # pydubë¡œ ë¡œë“œ
        original = AudioSegment.from_wav(io.BytesIO(original_wav_bytes))
        
        start_ms = int(start_sec * 1000)
        end_ms = int(end_sec * 1000)
        
        # ë²”ìœ„ ìœ íš¨ì„± ê²€ì‚¬
        if start_ms < 0: start_ms = 0
        if end_ms > len(original): end_ms = len(original)
        if start_ms >= end_ms: return None # ì‹œì‘ì´ ëë³´ë‹¤ ë’¤ë©´ ì‘ì—… ë¶ˆê°€
        
        # [ì‚­ì œ ë¡œì§] ì•ë¶€ë¶„ + ë’·ë¶€ë¶„ (ì¤‘ê°„ì„ ê±´ë„ˆëœ€)
        # Crossfade(0.05ì´ˆ)ë¥¼ ì‚´ì§ ì£¼ë©´ ëš ëŠê¸°ëŠ” ëŠë‚Œ ì—†ì´ ìì—°ìŠ¤ëŸ½ê²Œ ë¶™ìŠµë‹ˆë‹¤.
        part_front = original[:start_ms]
        part_back = original[end_ms:]
        
        # ë‹¨ìˆœíˆ ë”í•˜ê¸°(+)ë„ ê°€ëŠ¥í•˜ì§€ë§Œ, ì•„ì£¼ ì§§ì€ í¬ë¡œìŠ¤í˜ì´ë“œë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.
        final_audio = part_front.append(part_back, crossfade=20) 
        # crossfadeê°€ ì‹«ìœ¼ë©´: final_audio = part_front + part_back
        
        output_io = io.BytesIO()
        final_audio.export(output_io, format="wav")
        return output_io.getvalue()
        
    except Exception as e:
        print(f"Delete Logic Error: {e}")
        return None

def patch_audio_segment(original_wav_bytes, new_wav_bytes, start_sec, end_sec):
    try:
        # [ë°©ì–´ ë¡œì§] ë°ì´í„°ê°€ Noneì´ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ì¦‰ì‹œ ì¤‘ë‹¨
        if not original_wav_bytes or len(original_wav_bytes) < 100:
            print("Patch Skip: ì›ë³¸ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return None
        if not new_wav_bytes or len(new_wav_bytes) < 100:
            print("Patch Skip: ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return None

        # [í•µì‹¬ ë°©ì–´] ì‹œì‘ ì½”ë“œê°€ RIFFê°€ ì•„ë‹ˆë©´(íŠ¹íˆ 00 00 00 00ì´ë©´) ì¤‘ë‹¨
        if not original_wav_bytes.startswith(b'RIFF'):
            print(f"Patch Skip: ì›ë³¸ ë°ì´í„° í—¤ë” ì˜¤ë¥˜ (Start code: {original_wav_bytes[:4]})")
            return None
        if not new_wav_bytes.startswith(b'RIFF'):
            # ìƒˆ ë°ì´í„°ê°€ Raw PCMì¼ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ê²½ê³ ë§Œ í•˜ê³  ë„˜ì–´ê°€ê±°ë‚˜, Raw ë³€í™˜ ì‹œë„
            # ì—¬ê¸°ì„œëŠ” ì•ˆì „í•˜ê²Œ WAV í—¤ë”ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨ì‹œí‚´ (add_silence_paddingì„ ê±°ì³¤ë‹¤ë©´ í—¤ë”ê°€ ìˆì–´ì•¼ í•¨)
            print(f"Patch Skip: ìƒˆ ë°ì´í„° í—¤ë” ì˜¤ë¥˜ (Start code: {new_wav_bytes[:4]})")
            return None

        # 1. pydub ë¡œë“œ
        original = AudioSegment.from_wav(io.BytesIO(original_wav_bytes))
        replacement = AudioSegment.from_wav(io.BytesIO(new_wav_bytes))
        
        start_ms = int(start_sec * 1000)
        end_ms = int(end_sec * 1000)
        
        # 2. í¸ì§‘ ìˆ˜í–‰
        if start_ms < 0: start_ms = 0
        if end_ms > len(original): end_ms = len(original)
        
        part_a = original[:start_ms]
        part_c = original[end_ms:]
        
        fade_duration = 50 
        if len(replacement) < fade_duration * 2:
            final_audio = part_a + replacement + part_c
        else:
            final_audio = part_a.fade_out(fade_duration) + replacement.fade_in(fade_duration).fade_out(fade_duration) + part_c.fade_in(fade_duration)
        
        output_io = io.BytesIO()
        final_audio.export(output_io, format="wav")
        return output_io.getvalue()
        
    except Exception as e:
        print(f"Patch Process Error: {e}")
        return None

def get_transcription_with_timestamps(api_key, audio_path):
    """
    [ìµœì¢…_V3] ë¬¸ì¥ ë‹¨ìœ„ ê°•ì œ ë³‘í•©(Merger) + ê¸¸ì´ ì œí•œ(Clamping) ì ìš© ë²„ì „
    1. AIê°€ ì‰¼í‘œ(,)ë‚˜ í˜¸í¡ ë‹¨ìœ„ë¡œ ëŠì–´ì„œ ì£¼ë”ë¼ë„, ë§ˆì¹¨í‘œ(., ?, !)ê°€ ë‚˜ì˜¬ ë•Œê¹Œì§€ ë’·ë¬¸ì¥ê³¼ í•©ì²´í•©ë‹ˆë‹¤.
    2. ì˜¤ë””ì˜¤ ì‹¤ì œ ê¸¸ì´ë³´ë‹¤ ê¸´ ì‹œê°„ì´ ë‚˜ì˜¤ì§€ ì•Šë„ë¡ ê°•ì œ ì ˆì‚­í•©ë‹ˆë‹¤.
    """
    client = genai.Client(api_key=api_key)
    
    # 1. [ì‹¤ì¸¡] ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì •
    real_duration = 0.0
    try:
        with wave.open(audio_path, 'rb') as w:
            frames = w.getnframes()
            rate = w.getframerate()
            real_duration = frames / float(rate)
    except:
        real_duration = 600.0 # ì¸¡ì • ì‹¤íŒ¨ ì‹œ fallback

    with open(audio_path, "rb") as f: audio_bytes = f.read()

    # 2. [ì£¼ì…] í”„ë¡¬í”„íŠ¸ ì„¤ì •
    prompt = f"""
    Listen to the audio and transcribe it.
    
    **CONTEXT:**
    - Total duration: {real_duration:.2f} seconds.
    
    **TASK:**
    - Transcribe the audio into a JSON list of segments.
    - Ideally, split by FULL SENTENCES (ending with . ? !).
    - If there is a long pause, you might split earlier, but try to keep sentences together.
    
    **FORMAT:**
    [
      {{"start": 0.0, "end": 5.2, "text": "Sentence one."}},
      {{"start": 5.2, "end": 10.5, "text": "Sentence two."}}
    ]
    Use double quotes. Time in seconds (float).
    """
    
    safety_settings = [
        types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
        types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
        types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
        types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
    ]

    try:
        response = client.models.generate_content(
            model="gemini-2.5-pro", 
            contents=[
                types.Content(role="user", parts=[
                    types.Part.from_bytes(data=audio_bytes, mime_type="audio/wav"),
                    types.Part.from_text(text=prompt)
                ])
            ],
            config=types.GenerateContentConfig(safety_settings=safety_settings)
        )
        
        if not response.candidates or not response.candidates[0].content.parts:
            return []

        raw_text = response.text
        
        # JSON íŒŒì‹±
        start_idx = raw_text.find('[')
        end_idx = raw_text.rfind(']')
        if start_idx == -1 or end_idx == -1: return []
        clean_text = raw_text[start_idx : end_idx+1]

        # ì‹œê°„ í¬ë§· ë³´ì • (MM:SS -> SS)
        def time_replacer(match):
            time_str = match.group(0)
            if ':' in time_str:
                parts = time_str.split(':')
                if len(parts) == 2:
                    return str(int(parts[0]) * 60 + float(parts[1]))
                elif len(parts) == 3: 
                    return str(int(parts[0]) * 3600 + int(parts[1]) * 60 + float(parts[2]))
            return time_str 

        clean_text = re.sub(r'\d+:\d+(\.\d+)?', time_replacer, clean_text)
        
        # 1ì°¨ íŒŒì‹± ë°ì´í„°
        raw_data = json.loads(clean_text)
        
        # ---------------------------------------------------------
        # [í•µì‹¬ ë¡œì§] ë¬¸ì¥ ë³‘í•©ê¸° (Sentence Merger)
        # ë§ˆì¹¨í‘œë¡œ ëë‚˜ì§€ ì•ŠëŠ” ì¡°ê°ë“¤ì„ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤.
        # ---------------------------------------------------------
        merged_data = []
        buffer = None

        for item in raw_data:
            # ì‹œê°„ ìœ íš¨ì„± ì²´í¬ (ì‹œì‘ì´ ëë³´ë‹¤ ë’¤ê±°ë‚˜, ì‹œì‘ì´ ì „ì²´ ê¸¸ì´ ë„˜ìœ¼ë©´ íŒ¨ìŠ¤)
            if item['start'] >= item['end']: continue
            if item['start'] >= real_duration: continue
            
            # ë ì‹œê°„ ë³´ì • (Clamping)
            if item['end'] > real_duration: item['end'] = real_duration
            
            # ë²„í¼ê°€ ë¹„ì–´ìˆìœ¼ë©´ ìƒˆë¡œ ì‹œì‘
            if buffer is None:
                buffer = item
            else:
                # ë²„í¼ê°€ ìˆìœ¼ë©´ ì´ì–´ ë¶™ì´ê¸°
                # í…ìŠ¤íŠ¸ëŠ” ê³µë°± ì¶”ê°€í•´ì„œ ì—°ê²°, ë ì‹œê°„ì€ í˜„ì¬ ì•„ì´í…œì˜ ë ì‹œê°„ìœ¼ë¡œ í™•ì¥
                buffer['text'] = buffer['text'].strip() + " " + item['text'].strip()
                buffer['end'] = item['end']
            
            # í˜„ì¬ ë²„í¼ì˜ í…ìŠ¤íŠ¸ê°€ ë¬¸ì¥ ë¶€í˜¸ë¡œ ëë‚˜ëŠ”ì§€ í™•ì¸
            current_text = buffer['text'].strip()
            if current_text and current_text[-1] in ['.', '?', '!']:
                merged_data.append(buffer)
                buffer = None # ë²„í¼ ë¹„ìš°ê¸° (ë‹¤ìŒ ë¬¸ì¥ ì‹œì‘ ì¤€ë¹„)
        
        # ë£¨í”„ê°€ ëë‚¬ëŠ”ë° ë²„í¼ì— ë‚¨ì€ ê²Œ ìˆë‹¤ë©´ (ë§ˆì§€ë§‰ ë¬¸ì¥ì— ë§ˆì¹¨í‘œê°€ ì—†ëŠ” ê²½ìš° ë“±)
        if buffer is not None:
            merged_data.append(buffer)

        return merged_data
        
    except Exception as e:
        print(f"Parsing Error: {e}")
        return []

# ==========================================
# [3-0] ìœ í‹¸ë¦¬í‹°: ì˜¤ë””ì˜¤ í…ìŠ¤íŠ¸ ë™ê¸°í™”
# ==========================================
# [êµì²´] ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´ (JS ì œì–´ìš©)
def render_seekable_player(audio_path, seek_time=0.0):
    """
    ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´ë¥¼ ë Œë”ë§í•˜ê³ , seek_timeì´ ë³€ê²½ë˜ë©´ í•´ë‹¹ ìœ„ì¹˜ë¡œ ì´ë™ì‹œí‚µë‹ˆë‹¤.
    """
    with open(audio_path, "rb") as f:
        audio_b64 = base64.b64encode(f.read()).decode()
        
    # HTML/JS: ì˜¤ë””ì˜¤ ë¡œë“œ ì‹œ íŠ¹ì • ì‹œê°„ìœ¼ë¡œ ì í”„
    html_code = f"""
    <audio id="player" controls style="width: 100%;">
        <source src="data:audio/wav;base64,{audio_b64}" type="audio/wav">
    </audio>
    <script>
        var audio = document.getElementById('player');
        // í˜ì´ì§€ ë¡œë“œ(Rerun) ì‹œ íŒŒì´ì¬ì—ì„œ ë„˜ê²¨ì¤€ ì‹œê°„ìœ¼ë¡œ ì´ë™
        audio.currentTime = {seek_time};
        // ìë™ ì¬ìƒ (ì„ íƒ ì‚¬í•­, ì›ì¹˜ ì•Šìœ¼ë©´ ì£¼ì„ ì²˜ë¦¬)
        // audio.play(); 
    </script>
    """
    st.components.v1.html(html_code, height=60)

# ==========================================
# [3] ìœ í‹¸ë¦¬í‹°: ê²€ì¦ (STT & Diff)
# ==========================================
def normalize_text_strict(text):
    text = re.sub(r'[^ê°€-í£a-zA-Z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def compare_texts_diff(original, transcribed):
    clean_orig = normalize_text_strict(original)
    clean_trans = normalize_text_strict(transcribed)
    matcher = difflib.SequenceMatcher(None, clean_orig, clean_trans)
    similarity = matcher.ratio() * 100
    
    diff_html = []
    for tag, i1, i2, j1, j2 in matcher.get_opcodes():
        orig_fragment = clean_orig[i1:i2]
        trans_fragment = clean_trans[j1:j2]
        if tag == 'equal': diff_html.append(f"<span style='color:black; opacity:0.7;'>{orig_fragment}</span>")
        elif tag == 'replace': diff_html.append(f"<span style='color:red; text-decoration:line-through; background-color:#ffe6e6; font-weight:bold;'>{orig_fragment}</span> <span style='color:green; background-color:#e6ffe6; font-weight:bold;'>{trans_fragment}</span>")
        elif tag == 'delete': diff_html.append(f"<span style='color:red; text-decoration:line-through; background-color:#ffe6e6; font-weight:bold;'>{orig_fragment}</span>")
        elif tag == 'insert': diff_html.append(f"<span style='color:blue; background-color:#e6f3ff; font-weight:bold;'>{trans_fragment}</span>")
    return similarity, "".join(diff_html)

def verify_audio_content(api_key, audio_path):
    client = genai.Client(api_key=api_key)
    with open(audio_path, "rb") as f: audio_data = f.read()
    prompt = "Listen and transcribe EXACTLY as spoken."
    try:
        response = client.models.generate_content(
            model=VERIFY_MODEL_ID,
            contents=[types.Content(role="user", parts=[types.Part.from_bytes(data=audio_data, mime_type="audio/wav"), types.Part.from_text(text=prompt)])]
        )
        return response.text
    except Exception as e: return f"Error: {e}"

def get_correction_suggestion(api_key, original, transcribed):
    client = genai.Client(api_key=api_key)
    
    # í”„ë¡¬í”„íŠ¸: 'ì˜¤ë¥˜ íƒì§€' + 'í•´ê²°ì±…(ëŒ€ë³¸ ìˆ˜ì •ì•ˆ) ì œì‹œ'
    prompt = f"""
    ë‹¹ì‹ ì€ AI ì˜¤ë””ì˜¤ë¶ ì œì‘ì„ ìœ„í•œ **'ëŒ€ë³¸ ìˆ˜ì„  ì „ë¬¸ê°€(Script Doctor)'**ì…ë‹ˆë‹¤.
    [ì›ë³¸ í…ìŠ¤íŠ¸]ì™€ [STT(ë…¹ìŒëœ ë°œìŒ)]ë¥¼ ë¹„êµí•˜ì—¬, **ì¹˜ëª…ì ì¸ ë‚´ìš© ì˜¤ë¥˜**ë¥¼ ì°¾ê³ ,
    ë‹¤ìŒ ìƒì„± ì‹œ ì´ë¥¼ ë°©ì§€í•  ìˆ˜ ìˆëŠ” **êµ¬ì²´ì ì¸ ëŒ€ë³¸ ìˆ˜ì •ì•ˆ**ì„ ì œì‹œí•˜ì„¸ìš”.
    
    [ì›ë³¸ í…ìŠ¤íŠ¸]:
    {original}
    
    [STT (ë…¹ìŒëœ ë‚´ìš©)]:
    {transcribed}
    
    **[ë¶„ì„ ë° ì²˜ë°© ê°€ì´ë“œ]**
    1. **ëˆ„ë½(Omission) ë°œìƒ ì‹œ:**
       - **ì›ì¸:** ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ê±°ë‚˜ í˜¸í¡ì´ ê°€ë¹ ì„œ AIê°€ ê¸‰í•˜ê²Œ ë„˜ì–´ê°€ë©° ê±´ë„ˆëœ€.
       - **ğŸ’¡ ëŒ€ë³¸ ìˆ˜ì •ë²•:** ì‰¼í‘œ(,)ë‚˜ ë§ˆì¹¨í‘œ(.)ë¥¼ ì¶”ê°€í•˜ì—¬ ë¬¸ì¥ì„ ë¬¼ë¦¬ì ìœ¼ë¡œ ëŠì–´ì£¼ê±°ë‚˜, ì¤„ë°”ê¿ˆì„ í•˜ì„¸ìš”.
    2. **ì˜¤ë…(Mutation) ë°œìƒ ì‹œ:**
       - **ì›ì¸:** í•œìì–´, ì˜ì–´, ë˜ëŠ” ë°œìŒì´ ëª¨í˜¸í•œ ë‹¨ì–´.
       - **ğŸ’¡ ëŒ€ë³¸ ìˆ˜ì •ë²•:** ì†Œë¦¬ ë‚˜ëŠ” ëŒ€ë¡œ í•œê¸€ ë°œìŒì„ ì ê±°ë‚˜(ì˜ˆ: 'resume' -> 'ë ˆì¥¬ë©”'), í•œ ê¸€ìì”© ë„ì–´ì“°ê¸°(ì˜ˆ: 'í™•.ì‹¤.íˆ').
    3. **ìˆœì„œ ë’¤ë°”ë€œ/ë¬¸ë²• íŒŒê´´ ì‹œ:**
       - **ì›ì¸:** ë¬¸ì¥ êµ¬ì¡°ê°€ ë³µì¡í•˜ì—¬ AIê°€ ë¬¸ë§¥ì„ ì¬í•´ì„í•´ë²„ë¦¼.
       - **ğŸ’¡ ëŒ€ë³¸ ìˆ˜ì •ë²•:** ê¸´ ë¬¸ì¥ì„ ë‘ ê°œì˜ ì§§ì€ ë‹¨ë¬¸ìœ¼ë¡œ ìª¼ê°œì„¸ìš”.
    
    **[ì¶œë ¥ í˜•ì‹]**
    ì˜¤ë¥˜ê°€ ë°œê²¬ëœ ê²½ìš°ì—ë§Œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”. (ì™„ë²½í•˜ë©´ "âœ… ìˆ˜ì • ë¶ˆí•„ìš”" ì¶œë ¥)
    
    **1. [ì˜¤ë¥˜ ìœ í˜•]** (ê°„ë‹¨í•œ ì›ì¸ ì„¤ëª…)
       - ğŸ”´ **ë¬¸ì œ êµ¬ê°„:** "(ì˜¤ë¥˜ê°€ ë°œìƒí•œ ì›ë³¸ ë¬¸ì¥ ë¶€ë¶„)"
       - ğŸŸ¢ **ìˆ˜ì • ì œì•ˆ:** "(AIê°€ ì‹¤ìˆ˜í•˜ì§€ ì•Šë„ë¡ ê¸°í˜¸ë‚˜ ì² ìë¥¼ ë³€ê²½í•œ í…ìŠ¤íŠ¸)"
    
    **[ì‘ì„± ì˜ˆì‹œ]**
    **1. [ë‹¨ì–´ ëˆ„ë½]** ë¬¸ì¥ì´ ê¸¸ì–´ì„œ ë’·ë¶€ë¶„ 'ê°‘ìê¸°'ë¥¼ ê±´ë„ˆëœ€
       - ğŸ”´ **ë¬¸ì œ êµ¬ê°„:** "ê·¸ëŠ” ë¬¸ì„ ì—´ê³  ë“¤ì–´ì™€ì„œ ê°‘ìê¸° ì†Œë¦¬ì³¤ë‹¤"
       - ğŸŸ¢ **ìˆ˜ì • ì œì•ˆ:** "ê·¸ëŠ” ë¬¸ì„ ì—´ê³  ë“¤ì–´ì™€ì„œ, ê°‘ìê¸°, ì†Œë¦¬ì³¤ë‹¤" (ì‰¼í‘œë¡œ í˜¸í¡ ê°•ì œ)
       
    **2. [ì¹˜ëª…ì  ì˜¤ë…]** 'ì°½ì¡°ì 'ì„ 'ì°¸ì¡°ì 'ìœ¼ë¡œ ë°œìŒí•¨
       - ğŸ”´ **ë¬¸ì œ êµ¬ê°„:** "ì°½ì¡°ì ì¸ í™œë™"
       - ğŸŸ¢ **ìˆ˜ì • ì œì•ˆ:** "ì°½.ì¡°.ì ì¸ í™œë™" (ê°•ì¡°ì  í™œìš©)
    """
    
    try:
        response = client.models.generate_content(model=VERIFY_MODEL_ID, contents=[prompt])
        return response.text
    except: 
        return "ìˆ˜ì • ì œì•ˆ ë¶„ì„ ì‹¤íŒ¨"

def verify_errors_with_timestamp(api_key, audio_path, original_text):
    """
    [ê°•ë ¥ ê²€ì¦] ì˜¤ë””ì˜¤ë¥¼ ë¨¼ì € 'ë°›ì•„ì“°ê¸°' ì‹œí‚¨ í›„ ì›ë¬¸ê³¼ ëŒ€ì¡°í•˜ì—¬,
    AIê°€ ëŒ€ì¶© ë„˜ì–´ê°€ëŠ” ê²ƒì„ ë°©ì§€í•˜ê³  ì •ë°€í•˜ê²Œ ì˜¤ë¥˜ë¥¼ ì°¾ì•„ëƒ…ë‹ˆë‹¤.
    """
    client = genai.Client(api_key=api_key)
    
    with open(audio_path, "rb") as f:
        audio_bytes = f.read()
        
    # í”„ë¡¬í”„íŠ¸ ì „ëµ ë³€ê²½: "ë¹„êµí•´ë¼" (X) -> "ë°›ì•„ì“°ê³  í‹€ë¦° ê·¸ë¦¼ ì°¾ì•„ë¼" (O)
    prompt = f"""
    You are a paranoid Forensic Audio Analyst. 
    Your goal is to catch ANY discrepancy between the audio and the script.
    
    [Original Script]:
    {original_text}
    
    **INSTRUCTIONS (Follow Step-by-Step):**
    1. **Listen & Transcribe:** First, listen to the audio carefully and form a mental verbatim transcript of exactly what was said.
    2. **Compare:** Compare your mental transcript against the [Original Script] word by word.
    3. **Detect Errors:** Flag ANY difference, specifically:
       - **SKIPPED (Omission):** A word/sentence in the script is NOT in the audio.
       - **CHANGED (Mutation):** The script says "Father" but audio says "Brother".
       - **ADDED (Insertion):** Audio contains words not in the script.
       - **ORDER (Reordering):** Words are spoken in the wrong order.
    
    **STRICT RULES:**
    - DO NOT say "No errors found" if there is even a single word missing.
    - Be extremely strict. Do not forgive slight changes.
    - Provide the timestamp where the error STARTS.
    
    **Output Format (Markdown Table):**
    | Time (MM:SS) | Error Type | Script Text | Audio (What was heard) | Correction Suggestion |
    | :--- | :--- | :--- | :--- | :--- |
    | 00:05 | Omission | "He opened the door" | (Silence/Skipped) | Insert missing phrase |
    | 01:12 | Mutation | "Resume" | "Re-zoom" | Pronounce as "Re-zu-may" |
    
    If absolutely NO errors exist after checking twice, print: "âœ… **Perfect Match: Verified.**"
    """
    
    try:
        response = client.models.generate_content(
            model="gemini-2.5-pro", # ë§Œì•½ ì—¬ì „íˆ ëª» ì°¾ìœ¼ë©´ 'gemini-1.5-pro'ë¡œ ë³€ê²½ ê¶Œì¥
            contents=[
                types.Content(role="user", parts=[
                    types.Part.from_text(text="[Original Script]:"),
                    types.Part.from_text(text=original_text),
                    types.Part.from_text(text="[Audio File]:"),
                    types.Part.from_bytes(data=audio_bytes, mime_type="audio/wav"),
                    types.Part.from_text(text=prompt)
                ])
            ]
        )
        return response.text
    except Exception as e:
        return f"ê²€ì¦ ì‹¤íŒ¨: {e}"

def analyze_voice_similarity(api_key, ref_audio_bytes, target_audio_path):
    """
    [ê²€ì¦ìš©] ê¸°ì¤€ ì˜¤ë””ì˜¤(Reference)ì™€ ìƒì„±ëœ ì˜¤ë””ì˜¤(Generated)ë¥¼ ë¹„êµí•˜ì—¬
    ëª©ì†Œë¦¬ í†¤, ê°ì •, ì†ë„ ë“±ì˜ ì¼ì¹˜ìœ¨ì„ ë¶„ì„í•¨.
    """
    client = genai.Client(api_key=api_key)
    
    # ìƒì„±ëœ ì˜¤ë””ì˜¤ íŒŒì¼ ì½ê¸°
    with open(target_audio_path, "rb") as f:
        target_bytes = f.read()
        
    prompt = """
    Listen to the two audio files provided.
    Audio 1 is the [Reference Voice] (Ground Truth).
    Audio 2 is the [Generated Voice] (Target).
    
    Compare Audio 2 against Audio 1 specifically on:
    1. Tone & Emotion (Mood)
    2. Speaking Speed (Pacing)
    3. Voice Identity Similarity (Does it sound like the same person/style?)
    
    Provide a 'Similarity Score' (0-100%) and a brief explanation.
    
    Format:
    **Similarity Score:** XX%
    **Analysis:** (Brief reason in 1-2 sentences)
    """
    
    try:
        response = client.models.generate_content(
            model=ANALYZE_MODEL_ID,  # gemini-2.0-flash-exp (ë©€í‹°ëª¨ë‹¬ ì§€ì›)
            contents=[
                types.Content(role="user", parts=[
                    # [ìˆ˜ì •] text=... ë¥¼ ëª…ì‹œí•˜ì—¬ í‚¤ì›Œë“œ ì¸ìë¡œ ì „ë‹¬í•´ì•¼ í•¨
                    types.Part.from_text(text="Audio 1 (Reference):"),
                    types.Part.from_bytes(data=ref_audio_bytes, mime_type="audio/wav"),
                    types.Part.from_text(text="Audio 2 (Generated):"),
                    types.Part.from_bytes(data=target_bytes, mime_type="audio/wav"),
                    types.Part.from_text(text=prompt)
                ])
            ]
        )
        return response.text
    except Exception as e:
        return f"ë¶„ì„ ì‹¤íŒ¨: {e} (API í‚¤ë‚˜ ëª¨ë¸ ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”)"

def analyze_voice_style_to_prompt(api_key, ref_audio_bytes):
    """
    [2ë‹¨ê³„ìš©] ì˜¤ë””ì˜¤ë¥¼ ë“£ê³  ê·¸ íŠ¹ì§•ì„ 'ì—°ê¸° ì§€ì‹œ í…ìŠ¤íŠ¸'ë¡œ ë³€í™˜
    """
    client = genai.Client(api_key=api_key)
    
    prompt = """
    You are an expert Voice Engineer and Acting Coach.
    Listen to the attached audio file deeply.
    
    Analyze the voice waveform characteristics and convert them into specific acting instructions (System Prompt) for an AI TTS model.
    
    Focus on these specific details:
    1. **Pitch & Range**: (e.g., "Very Low Bass", "Mid-range Baritone", "High Soprano")
    2. **Timbre/Texture**: (e.g., "Raspy", "Breathiy", "Clear", "Gravelly")
    3. **Pacing & Rhythm**: (e.g., "Staccato (choppy)", "Legato (smooth)", "Fast-paced", "Slow and deliberate")
    4. **Intonation**: (e.g., "Flat/Monotone", "Dynamic/Expressive", "Rising at the end")
    
    **Output Instruction (in Korean):**
    Write a concise but highly descriptive instruction that forces the AI to mimic this exact sound. 
    Start with: "ë‹¹ì‹ ì€ [ì„±ê²©/íŠ¹ì§•] ëª©ì†Œë¦¬ë¥¼ ê°€ì§„ ì„±ìš°ì…ë‹ˆë‹¤..."
    """
    
    try:
        response = client.models.generate_content(
            model="gemini-3-pro-preview", # ë¶„ì„ì€ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì‚¬ìš©
            contents=[
                types.Content(role="user", parts=[
                    types.Part.from_text(text="Reference Audio:"),
                    types.Part.from_bytes(data=ref_audio_bytes, mime_type="audio/wav"),
                    types.Part.from_text(text=prompt)
                ])
            ]
        )
        return response.text.strip()
    except Exception as e:
        return f"ë¶„ì„ ì‹¤íŒ¨: {e}"

# ==========================================
# [4] ë©”ì¸ UI
# ==========================================
def main():
    st.set_page_config(layout="wide", page_title="AI AudioBook Workstation")
    
    st.sidebar.header("âš™ï¸ í™˜ê²½ ì„¤ì •")
    api_key = st.sidebar.text_input("Google API Key", type="password")
    if api_key: os.environ["GOOGLE_API_KEY"] = api_key
    
    st.title("ğŸ™ï¸ AI AudioBook : 3-Step Workstation")
    
    tab1, tab2, tab3 = st.tabs(["1ï¸âƒ£ í…ìŠ¤íŠ¸ ì¶”ì¶œ & ëŒ€ë³¸í™”", "2ï¸âƒ£ ì˜¤ë””ì˜¤ ìë™ ìƒì„± (Batch)", "3ï¸âƒ£ í’ˆì§ˆ ê²€ì¦ & ìˆ˜ì •"])

    # ----------------------------------------------------------------
    # TAB 1: í…ìŠ¤íŠ¸ ë¶„í•  (UI ì›ë³µ: ì¢Œìš° ë¶„í• )
    # ----------------------------------------------------------------
    with tab1:
        st.subheader("1ë‹¨ê³„: EPUB ì¶”ì¶œ ë° ëŒ€ë³¸ íŒŒì¼ ìƒì„±")
        
        # [A] íŒŒì¼ ì—…ë¡œë“œ ë° ë¡œë“œ
        col_load1, col_load2 = st.columns(2)
        with col_load1:
            uploaded_file = st.file_uploader("EPUB íŒŒì¼ ì—…ë¡œë“œ", type=["epub"])
        with col_load2:
            if os.path.exists(REMAINING_FILE):
                st.info("ğŸ“‚ ì´ì „ì— ì‘ì—…í•˜ë˜ í…ìŠ¤íŠ¸ê°€ ìˆìŠµë‹ˆë‹¤.")
                if st.button("ì´ì–´í•˜ê¸° (ì €ì¥ëœ í…ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°)"):
                    st.session_state['full_source'] = load_text_file(REMAINING_FILE)
                    st.rerun()

        if uploaded_file and st.button("í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘"):
            text = get_full_text_from_epub(uploaded_file)
            st.session_state['full_source'] = text
            save_text_file(REMAINING_FILE, text)
            st.success("ì¶”ì¶œ ì™„ë£Œ!")
            st.rerun()

        # [B] ì¢Œìš° 2ë‹¨ ë¶„í•  UI
        if 'full_source' in st.session_state:
            st.divider()
            
            # ì¢Œ: ì›ë³¸ / ìš°: ì±•í„°ìƒì„±
            col_src, col_dst = st.columns([1, 1])
            
            # --- ì¢Œì¸¡: ì›ë³¸ í…ìŠ¤íŠ¸ ì—ë””í„° ---
            with col_src:
                st.markdown("### ğŸ“œ ì›ë³¸ í…ìŠ¤íŠ¸ (Ctrl+X)")
                # keyë¥¼ ì§€ì •í•˜ì—¬ ìˆ˜ì • ì‹œ session_stateì— ìë™ ë°˜ì˜
                new_source = st.text_area(
                    "Source Text", 
                    value=st.session_state['full_source'], 
                    height=600, 
                    label_visibility="collapsed",
                    key="source_editor"
                )
                # í…ìŠ¤íŠ¸ê°€ ë³€ê²½ë˜ë©´ ë³€ìˆ˜ì— ë°˜ì˜ (Streamlit ë™ì‘ íŠ¹ì„±ìƒ key ë°”ì¸ë”©ìœ¼ë¡œ ìë™ ì²˜ë¦¬ë¨)

            # --- ìš°ì¸¡: ì±•í„° ìƒì„± í¼ ---
            with col_dst:
                st.markdown("### âœ‚ï¸ ì±•í„° ìƒì„± (Ctrl+V)")
                
                with st.form("split_form"):
                    next_num = len(get_subdirectories(SCRIPT_ROOT)) + 1
                    
                    # [UI ì¶”ê°€] 1. ì±•í„° ì œëª©
                    chapter_title = st.text_input("ì±•í„° ì œëª©", f"Chapter {next_num:02d}")
                    
                    # [UI ì¶”ê°€] 2. ë¶„í•  ê¸€ì ìˆ˜ ì„¤ì • (ìŠ¬ë¼ì´ë”/ìˆ«ìì…ë ¥)
                    # ê¸°ë³¸ê°’ì„ 500ìë¡œ ì„¤ì • (ì˜¤ë¥˜ ê´€ë¦¬í•˜ê¸° ë”± ì¢‹ì€ ë¬¸ë‹¨ ë‹¨ìœ„ í¬ê¸°)
                    split_limit = st.number_input(
                        "âœ‚ï¸ íŒŒì¼ë‹¹ ìµœëŒ€ ê¸€ì ìˆ˜ (ê¶Œì¥: 300~600ì)", 
                        min_value=100, 
                        max_value=3000, 
                        value=500, 
                        step=50,
                        help="ì´ ê¸€ì ìˆ˜ë¥¼ ë„˜ì–´ê°€ë©´ ë¬¸ì¥(.) ë‹¨ìœ„ë¡œ ì˜ë¼ì„œ ë‹¤ìŒ íŒŒì¼ë¡œ ë„˜ê¹ë‹ˆë‹¤.\nì‘ì„ìˆ˜ë¡ ìˆ˜ì • ê´€ë¦¬ê°€ ì‰½ê³ , í´ìˆ˜ë¡ ë¬¸ë§¥ ì—°ê²°ì´ ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤."
                    )

                    # [UI ì¶”ê°€] 3. ë‚´ìš© ì…ë ¥
                    chapter_content = st.text_area("ì±•í„° ë‚´ìš© (ì—¬ê¸°ì— ë¶™ì—¬ë„£ê¸°)", height=450)
                    
                    submit_btn = st.form_submit_button("ğŸ’¾ ì±•í„° ì €ì¥ ë° ë³€í™˜")

                    if submit_btn:
                        if not chapter_content.strip():
                            st.error("ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                        else:
                            # 1. í´ë” ë° íŒŒì¼ ìƒì„±
                            safe_title = sanitize_filename(f"{next_num:02d}_{chapter_title}")
                            path = os.path.join(SCRIPT_ROOT, safe_title)
                            os.makedirs(path, exist_ok=True)
                            
                            # ì›ë³¸ ì €ì¥
                            save_text_file(os.path.join(path, "raw.txt"), chapter_content)
                            
                            # 2. ì „ì²˜ë¦¬ ë° ë¶„í•  (ì‚¬ìš©ìê°€ ì…ë ¥í•œ split_limit ì ìš©)
                            # process_text_for_playground í•¨ìˆ˜ì— max_chars ì¸ìë¥¼ ì „ë‹¬
                            chunks = process_text_for_playground(chapter_content, max_chars=split_limit)
                            
                            for i, chunk in enumerate(chunks):
                                base = f"script_{i+1:03d}"
                                save_text_file(os.path.join(path, f"{base}.txt"), chunk)
                                with open(os.path.join(path, f"{base}.json"), "w", encoding="utf-8") as f:
                                    json.dump({"segments": [{"text": chunk}]}, f, ensure_ascii=False, indent=2)
                            
                            # 3. ë‚¨ì€ ì›ë³¸ í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ (ì¢Œì¸¡ ì—ë””í„° ë‚´ìš© ì €ì¥)
                            st.session_state['full_source'] = new_source
                            save_text_file(REMAINING_FILE, new_source)
                            
                            st.success(f"âœ… '{safe_title}' ìƒì„± ì™„ë£Œ! (ì„¤ì •: {split_limit}ì ë‹¨ìœ„ â†’ ì´ {len(chunks)}ê°œ íŒŒì¼)")
                            st.rerun()


    # ----------------------------------------------------------------
    # TAB 2: ì˜¤ë””ì˜¤ ìƒì„± (ì•ˆì „í•œ ë³µì‚¬-ë¶™ì—¬ë„£ê¸° ë°©ì‹ ì ìš©)
    # ----------------------------------------------------------------
    with tab2:
        st.subheader("2ë‹¨ê³„: AI ì˜¤ë””ì˜¤ ì¼ê´„ ìƒì„±")
        
        # [A] í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
        PROMPT_TEMPLATES = {
            "ì§ì ‘ ì‘ì„± (Direct Input)": "",
            
            "ì¶”ì²œ 1: ğŸ­ ê°ì„± ì˜¤ë””ì˜¤ë¶ (ì†Œì„¤/ë¬¸í•™)": """ë‹¹ì‹ ì€ ë² í…Œë‘ ì˜¤ë””ì˜¤ë¶ ì„±ìš°ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ ë‹¨ìˆœíˆ ë‚­ë…í•˜ì§€ ë§ê³ , ì²­ìê°€ ì´ì•¼ê¸° ì† ì¥ë©´ì— ëª°ì…í•  ìˆ˜ ìˆë„ë¡ 'ì—°ê¸°'í•´ì£¼ì„¸ìš”.
[ì§€ì¹¨]
1. ëŒ€í™”ë¬¸(" ")ì€ ë“±ì¥ì¸ë¬¼ì˜ ì„±ê²©ì— ë§ì¶° ëª©ì†Œë¦¬ í†¤ì„ ì‚´ì§ ë°”ê¿”ì£¼ì„¸ìš”.
2. ì§€ë¬¸(í•´ì„¤)ì€ ì°¨ë¶„í•˜ê³  ë”°ëœ»í•œ ì–´ì¡°ë¡œ ì½ì–´ì£¼ì„¸ìš”.
3. ì‰¼í‘œë‚˜ ë§ˆì¹¨í‘œì—ì„œëŠ” ì¶©ë¶„íˆ í˜¸í¡ì„ ë‘ì–´, ì—¬ìœ ë¥¼ ì£¼ì„¸ìš”.""",
            
            "ì¶”ì²œ 2: ğŸ™ï¸ ì‹¬ì•¼ ë¼ë””ì˜¤ DJ (ì—ì„¸ì´/í¸ì•ˆí•¨)": """ë‹¹ì‹ ì€ ì‹¬ì•¼ ë¼ë””ì˜¤ í”„ë¡œê·¸ë¨ì˜ ë”°ëœ»í•œ DJì…ë‹ˆë‹¤. ì²­ìì—ê²Œ 1:1ë¡œ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ë“¯ì´ ë¶€ë“œëŸ½ê³  ì¹œê·¼í•˜ê²Œ ë§í•´ì£¼ì„¸ìš”.
[ì§€ì¹¨]
- ë¬¸ì¥ì„ ëë§ºì„ ë•Œ ë”±ë”±í•˜ê²Œ ëŠì§€ ë§ê³ , ìì—°ìŠ¤ëŸ½ê²Œ ì—¬ìš´ì„ ë‚¨ê²¨ì£¼ì„¸ìš”.
- ë°œìŒì€ ì •í™•í•´ì•¼ í•˜ì§€ë§Œ, ì•„ë‚˜ìš´ì„œì²˜ëŸ¼ ë”±ë”±í•˜ì§€ ì•Šê²Œ 'ëŒ€í™”í•˜ë“¯' ìì—°ìŠ¤ëŸ¬ì›Œì•¼ í•©ë‹ˆë‹¤.
- ì „ì²´ì ìœ¼ë¡œ ì°¨ë¶„í•˜ê³  ì•ˆì •ê° ìˆëŠ” 'ì¤‘ì €ìŒ'ì˜ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.""",
            
            "ì¶”ì²œ 3: ğŸ¬ ì˜í™” ê°™ì€ ì—°ì¶œ (íŒíƒ€ì§€/ëª°ì…)": """ì´ í…ìŠ¤íŠ¸ëŠ” ì˜í™”ì˜ í•œ ì¥ë©´ì…ë‹ˆë‹¤. ë‹¹ì‹ ì€ ì´ ì¥ë©´ì„ ëª©ì†Œë¦¬ë¡œ ì—°ê¸°í•˜ëŠ” ë°°ìš°ì…ë‹ˆë‹¤. ìƒí™© ë¬˜ì‚¬ì™€ ëŒ€ì‚¬ í•˜ë‚˜í•˜ë‚˜ì— ë‹´ê¸´ ê°ì •ì„ (ê¸´ì¥ê°, ê¸°ì¨, ìŠ¬í”” ë“±)ì„ ìƒìƒí•˜ê²Œ í‘œí˜„í•´ì£¼ì„¸ìš”.
[ì§€ì¹¨]
- ë‹¨ìˆœíˆ ê¸€ìë¥¼ ì½ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, í…ìŠ¤íŠ¸ ë’¤ì— ìˆ¨ê²¨ì§„ ê°ì •ì„ ì½ì–´ë‚´ì•¼ í•©ë‹ˆë‹¤.
- ìƒí™©ì´ ê¸´ë°•í•˜ë©´ í…œí¬ë¥¼ ì¡°ê¸ˆ ë¹ ë¥´ê²Œ, ì°¨ë¶„í•œ ìƒí™©ì´ë©´ ëŠë¦¬ê²Œ ì¡°ì ˆí•˜ì—¬ ë¦¬ë“¬ê°ì„ ë§Œë“œì„¸ìš”."""
        }

        # [B] í…œí”Œë¦¿ ë³€ê²½ ì½œë°± (ë“œë¡­ë‹¤ìš´ ë°”ê¿€ ë•Œë§Œ ì…ë ¥ì°½ ê°±ì‹ )
        def on_template_change():
            selected = st.session_state["template_selector"]
            new_text = PROMPT_TEMPLATES[selected]
            if selected == "ì§ì ‘ ì‘ì„± (Direct Input)" and not new_text:
                new_text = "ì°¨ë¶„í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì½ì–´ì£¼ì„¸ìš”."
            st.session_state["tab2_prompt"] = new_text

        # í´ë” ë¡œë“œ ë° ê²½ë¡œ ì„¤ì •
        folders = get_subdirectories(SCRIPT_ROOT)
        if not folders:
            st.warning("ìƒì„±ëœ ëŒ€ë³¸ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            sel_folder = st.selectbox("ì‘ì—…í•  ì±•í„° í´ë”", folders)
            script_path = os.path.join(SCRIPT_ROOT, sel_folder)
            audio_path = os.path.join(AUDIO_ROOT, sel_folder)
            os.makedirs(audio_path, exist_ok=True)
            
            txt_files = get_files_in_dir(script_path, ".txt")
            txt_files = [f for f in txt_files if f != "raw.txt"]
            existing_audios = get_files_in_dir(audio_path, ".wav")
            start_idx = len(existing_audios)
            
            col_info, col_set = st.columns([1, 2])
            with col_info:
                st.info(f"ğŸ“„ ì´ ëŒ€ë³¸ íŒŒì¼: {len(txt_files)}ê°œ")
                st.warning(f"ğŸš€ ìƒì„± ëŒ€ê¸°ì¤‘: {len(txt_files) - start_idx}ê°œ")
            
            with col_set:
                st.markdown("#### ğŸ›ï¸ ìƒì„± ì„¤ì •")
                c1, c2 = st.columns(2)
                
                with c1:
                    voice = st.selectbox("ì„±ìš° ì„ íƒ", VOICE_OPTIONS, index=0)
                    
                    # [ì˜¨ë„ì™€ ì†ë„ë¥¼ ë‚˜ë€íˆ ë°°ì¹˜í•˜ê±°ë‚˜ ìœ„ì•„ë˜ë¡œ ë°°ì¹˜]
                    col_opt1, col_opt2, col_opt3 = st.columns(3) # ì»¬ëŸ¼ì„ 3ê°œë¡œ ëŠ˜ë¦¼
                    with col_opt1:
                        temp = st.slider("ê°ì • ì˜¨ë„", 0.0, 2.0, 1.0, 0.1)
                    with col_opt2:
                        speed = st.slider("ì†ë„ (ë°°ì†)", 0.5, 2.0, 1.0, 0.1)
                    with col_opt3:
                        # [ì¶”ê°€ë¨] ë³¼ë¥¨ ìŠ¬ë¼ì´ë”
                        volume = st.slider("ë³¼ë¥¨ (dB)", -10.0, 10.0, 0.0, 1.0, help="+3dBëŠ” ì†Œë¦¬ê°€ 1.4ë°° ì»¤ì§‘ë‹ˆë‹¤.")
                    
                    # í…œí”Œë¦¿ ì„ íƒ (ë³€ê²½ ì‹œì—ë§Œ ì½œë°± ì‹¤í–‰)
                    st.selectbox(
                        "í”„ë¡¬í”„íŠ¸ í”„ë¦¬ì…‹", 
                        list(PROMPT_TEMPLATES.keys()), 
                        index=1,
                        key="template_selector",
                        on_change=on_template_change
                    )
                
                with c2:
                    # ì´ˆê¸°ê°’ ì„¸íŒ…
                    if "tab2_prompt" not in st.session_state:
                        st.session_state["tab2_prompt"] = PROMPT_TEMPLATES["ì¶”ì²œ 1: ğŸ­ ê°ì„± ì˜¤ë””ì˜¤ë¶ (ì†Œì„¤/ë¬¸í•™)"]

                    # ë©”ì¸ í”„ë¡¬í”„íŠ¸ ì…ë ¥ì°½
                    prompt_text = st.text_area("ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì—°ê¸° ì§€ì‹œ)", key="tab2_prompt", height=200)

                # ---------------------------------------------------------
                # [2ë‹¨ê³„ ì „ìš©] ì˜¤ë””ì˜¤ ìŠ¤íƒ€ì¼ ë¶„ì„ê¸° (ë³„ë„ ê²°ê³¼ í‘œì‹œ)
                # ---------------------------------------------------------
                st.markdown("---")
                st.markdown("##### ğŸ¤ ëª©ì†Œë¦¬ ìŠ¤íƒ€ì¼ ì¶”ì¶œ (ë”°ë¼í•˜ê¸°)")
                
                col_a1, col_a2 = st.columns([2, 3])
                with col_a1:
                    style_audio = st.file_uploader("ë”°ë¼í•  ëª©ì†Œë¦¬ ì—…ë¡œë“œ (ë¶„ì„ìš©)", type=["wav", "mp3"], key="style_ref_tab2")
                
                with col_a2:
                    st.write("") 
                    st.write("")
                    
                    # ë¶„ì„ ê²°ê³¼ ì €ì¥ìš© ì„¸ì…˜ ë³€ìˆ˜
                    if "analysis_result_text" not in st.session_state:
                        st.session_state["analysis_result_text"] = ""

                    if style_audio:
                        if st.button("ìŠ¤íƒ€ì¼ ë¶„ì„ ì‹¤í–‰ ğŸ”"):
                            if not api_key: st.error("API Key í•„ìš”")
                            else:
                                with st.spinner("ëª©ì†Œë¦¬ ë¶„ì„ ì¤‘..."):
                                    extracted_style = analyze_voice_style_to_prompt(api_key, style_audio.getvalue())
                                    st.session_state["analysis_result_text"] = extracted_style
                                    st.success("ë¶„ì„ ì™„ë£Œ! ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ ë³µì‚¬í•´ì„œ ìœ„ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€í•˜ì„¸ìš”.")

                # ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í‘œì‹œ (ì‚¬ìš©ìê°€ ë³µì‚¬í•  ìˆ˜ ìˆê²Œ)
                if st.session_state["analysis_result_text"]:
                    st.info("ğŸ‘‡ **ë¶„ì„ëœ ìŠ¤íƒ€ì¼ ì§€ì‹œë¬¸ (ë³µì‚¬í•´ì„œ ìœ„ì˜ í”„ë¡¬í”„íŠ¸ ì°½ì— ë¶™ì—¬ë„£ìœ¼ì„¸ìš”)**")
                    st.code(st.session_state["analysis_result_text"], language="text")

                st.markdown("---")
                batch_count = st.number_input("ì´ë²ˆì— ìƒì„±í•  ê°œìˆ˜", min_value=1, max_value=max(1, len(txt_files)-start_idx), value=min(5, max(1, len(txt_files)-start_idx)))

            if st.button("ğŸ™ï¸ ì˜¤ë””ì˜¤ ìƒì„± ì‹œì‘ (ì´ì–´í•˜ê¸°)"):
                if not api_key:
                    st.error("API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                else:
                    # [NEW] ì„¤ì •ê°’ ì €ì¥ (Metadata Saving)
                    # ë‚˜ì¤‘ì— 3ë‹¨ê³„ì—ì„œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆë„ë¡ ì„¤ì •ê°’ì„ JSONìœ¼ë¡œ ë°•ì œí•©ë‹ˆë‹¤.
                    settings_data = {
                        "voice": voice,
                        "speed": speed,
                        "temperature": temp,
                        "prompt": prompt_text
                    }
                    with open(os.path.join(audio_path, "settings.json"), "w", encoding="utf-8") as f:
                        json.dump(settings_data, f, ensure_ascii=False, indent=2)
                    
                    st.toast("âš™ï¸ ìƒì„± ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

                    # --- ê¸°ì¡´ ìƒì„± ë¡œì§ ---
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    target_files = txt_files[start_idx : start_idx + batch_count]
                    
                    if not target_files:
                         st.info("ìƒì„±í•  ëŒ€ìƒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        for i, txt_file in enumerate(target_files):
                            current_idx = start_idx + i + 1
                            status_text.markdown(f"**[{current_idx}/{len(txt_files)}] ìƒì„± ì¤‘...** `{txt_file}`")
                            text_content = load_text_file(os.path.join(script_path, txt_file))
                            
                            try:
                                audio_bytes = generate_speech(api_key, text_content, prompt_text, voice, temp, speed)
                                
                                if audio_bytes:
                                    final_audio = add_silence_padding(audio_bytes)
                                    if final_audio:
                                        save_name = txt_file.replace("script_", "audio_").replace(".txt", ".wav")
                                        with open(os.path.join(audio_path, save_name), "wb") as f:
                                            f.write(final_audio)
                                        status_text.markdown(f"âœ… **ì„±ê³µ:** `{save_name}`")
                                    else:
                                        st.error(f"íŒ¨ë”© ì˜¤ë¥˜: {txt_file}")
                                else:
                                    st.error(f"ìƒì„± ì‹¤íŒ¨: {txt_file}")
                                    break
                            except Exception as e:
                                st.error(f"ì—ëŸ¬: {e}")
                                break
                            
                            progress_bar.progress((i + 1) / batch_count)
                            time.sleep(1)
                        st.success("ì™„ë£Œ!")
                        time.sleep(2)
                        st.rerun()


    # ----------------------------------------------------------------
    # TAB 3: ê²€ì¦ ë° ìˆ˜ì • (í´ë¦­ ì—°ë™ ì—…ê·¸ë ˆì´ë“œ ë²„ì „)
    # ----------------------------------------------------------------
    with tab3:
        st.subheader("3ë‹¨ê³„: í’ˆì§ˆ ê²€ì¦ ë° ì •ë°€ ìˆ˜ì„ ")
        
        folders = get_subdirectories(SCRIPT_ROOT)
        if not folders:
            st.warning("ì‘ì—…í•  í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            sel_folder = st.selectbox("ê²€ì¦í•  í´ë”", folders, key="verify_folder")
            script_path = os.path.join(SCRIPT_ROOT, sel_folder)
            audio_path = os.path.join(AUDIO_ROOT, sel_folder)
            
            # ì„¤ì • íŒŒì¼ ë¡œë“œ
            loaded_voice_idx = 0
            loaded_speed = 1.0
            loaded_prompt = "ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°ë˜ë„ë¡ ì½ì–´ì£¼ì„¸ìš”."
            
            settings_file = os.path.join(audio_path, "settings.json")
            if os.path.exists(settings_file):
                try:
                    with open(settings_file, "r", encoding="utf-8") as f:
                        saved_settings = json.load(f)
                    if saved_settings.get("voice") in VOICE_OPTIONS:
                        loaded_voice_idx = VOICE_OPTIONS.index(saved_settings.get("voice"))
                    loaded_speed = float(saved_settings.get("speed", 1.0))
                    loaded_prompt = saved_settings.get("prompt", "")
                except: pass

            json_files = get_files_in_dir(script_path, ".json")
            if json_files:
                target_script = st.selectbox("ëŒ€ë³¸ íŒŒì¼ ì„ íƒ", json_files)
                target_audio = target_script.replace("script_", "audio_").replace(".json", ".wav")
                audio_full_path = os.path.join(audio_path, target_audio)

                # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
                if "input_start" not in st.session_state: st.session_state["input_start"] = 0.0
                if "input_end" not in st.session_state: st.session_state["input_end"] = 0.0
                if "input_text" not in st.session_state: st.session_state["input_text"] = ""
                
                # íƒ€ì„ë¼ì¸ ë°ì´í„° í™•ë³´
                verify_key = f"timeline_{target_script}"
                if verify_key not in st.session_state:
                    if st.button("ğŸš€ ë¶„ì„ ë°ì´í„° ìƒì„± (ìµœì´ˆ 1íšŒ í•„ìˆ˜)"):
                        if not api_key: st.error("API Key í•„ìš”")
                        else:
                            with st.spinner("ì˜¤ë””ì˜¤ ë¶„ì„ ì¤‘..."):
                                timeline_data = get_transcription_with_timestamps(api_key, audio_full_path)
                                st.session_state[verify_key] = timeline_data
                                st.rerun()

                st.divider()

                # [í™”ë©´ ë¶„í• ] ì¢Œ: ë¦¬ìŠ¤íŠ¸(ì„ íƒ) / ìš°: ì—ë””í„°
                col_list, col_edit = st.columns([1.2, 1])
                
                # ==========================================
                # [ì¢Œì¸¡] ì¸í„°ë™í‹°ë¸Œ ë¦¬ìŠ¤íŠ¸ (í´ë¦­ -> ì…ë ¥ ì—°ë™)
                # ==========================================
                with col_list:
                    st.markdown("### ğŸ“œ ë¬¸ì¥ ì„ íƒ (í´ë¦­)")
                    
                    if os.path.exists(audio_full_path):
                        # 1. ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´ (í˜„ì¬ ì„ íƒëœ ì‹œê°„ìœ¼ë¡œ ë Œë”ë§)
                        render_seekable_player(audio_full_path, seek_time=st.session_state["input_start"])
                        
                        # 2. ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ (ë²„íŠ¼)
                        if verify_key in st.session_state and st.session_state[verify_key]:
                            timeline = st.session_state[verify_key]
                            
                            # ìŠ¤í¬ë¡¤ ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆ (ë†’ì´ ê³ ì •)
                            with st.container(height=500):
                                for i, seg in enumerate(timeline):
                                    # í˜„ì¬ ì„ íƒëœ ë¬¸ì¥ì´ë©´ ìƒ‰ìƒ ê°•ì¡° (primary)
                                    is_selected = (seg['start'] == st.session_state["input_start"])
                                    btn_type = "primary" if is_selected else "secondary"
                                    
                                    # ë²„íŠ¼ ë¼ë²¨: [ì‹œê°„] í…ìŠ¤íŠ¸
                                    label = f"[{seg['start']:.1f}s ~ {seg['end']:.1f}s] {seg['text']}"
                                    
                                    # ë²„íŠ¼ í´ë¦­ ì‹œ -> ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸ -> Rerun
                                    if st.button(label, key=f"btn_{i}", type=btn_type, use_container_width=True):
                                        st.session_state["input_start"] = seg['start']
                                        st.session_state["input_end"] = seg['end']
                                        st.session_state["input_text"] = seg['text']
                                        st.rerun()
                        else:
                            st.info("ğŸ‘† ìœ„ 'ë¶„ì„ ë°ì´í„° ìƒì„±' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
                    else:
                        st.error("ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

                # ==========================================
                # [ìš°ì¸¡] ì˜¤ë””ì˜¤ í¸ì§‘ (ìë™ ì‹±í¬ ë³´ì • ì ìš©ë¨)
                # ==========================================
                with col_edit:
                    st.markdown("### âœ‚ï¸ 1. ì—¬ë°±/êµ¬ê°„ ì‚­ì œ")
                    dc1, dc2 = st.columns(2)
                    with dc1:
                        del_start = st.number_input("ì‚­ì œ ì‹œì‘", min_value=0.0, step=0.1, format="%.2f", key="del_start")
                    with dc2:
                        del_end = st.number_input("ì‚­ì œ ë", min_value=0.0, step=0.1, format="%.2f", key="del_end")
                    
                    if st.button("ğŸ—‘ï¸ êµ¬ê°„ ì‚­ì œ ì‹¤í–‰", type="primary", use_container_width=True):
                         if os.path.exists(audio_full_path):
                            with open(audio_full_path, "rb") as f: original_bytes = f.read()
                            
                            new_audio_bytes = delete_audio_range(original_bytes, del_start, del_end)
                            
                            if new_audio_bytes:
                                # 1. ì˜¤ë””ì˜¤ ì €ì¥
                                with open(audio_full_path, "wb") as f: f.write(new_audio_bytes)
                                
                                # 2. [í•µì‹¬] íƒ€ì„ë¼ì¸ ë°ì´í„° ì¦‰ì‹œ ë³´ì • (API í˜¸ì¶œ X)
                                if verify_key in st.session_state:
                                    old_timeline = st.session_state[verify_key]
                                    # ìˆ˜í•™ì  ê³„ì‚°ìœ¼ë¡œ ì‹œê°„ ë‹¹ê¸°ê¸°
                                    new_timeline = adjust_timeline_for_deletion(old_timeline, del_start, del_end)
                                    st.session_state[verify_key] = new_timeline
                                    st.toast(f"âœ… ì˜¤ë””ì˜¤ ì‚­ì œ ì™„ë£Œ! íƒ€ì„ë¼ì¸ë„ {del_end-del_start:.1f}ì´ˆ ë‹¹ê²¨ì¡ŒìŠµë‹ˆë‹¤.")
                                else:
                                    st.success("ì‚­ì œ ì™„ë£Œ")
                                
                                time.sleep(0.5)
                                st.rerun()
                            else:
                                st.error("ì‚­ì œ ì‹¤íŒ¨")

                    st.divider()

                    st.markdown("### ğŸ› ï¸ 2. ë¬¸ì¥ ì¬ë…¹ìŒ")
                    tc1, tc2 = st.columns(2)
                    with tc1:
                        start_time = st.number_input("ì‹œì‘(ì´ˆ)", step=0.1, format="%.2f", key="input_start")
                    with tc2:
                        end_time = st.number_input("ë(ì´ˆ)", step=0.1, format="%.2f", key="input_end")
                    
                    patch_text = st.text_area("ìˆ˜ì •í•  ë‚´ìš©", height=100, key="input_text")

                    with st.expander("âš™ï¸ ì„±ìš° / ì†ë„ / ë³¼ë¥¨ ì„¤ì •", expanded=False):
                        pc1, pc2, pc3 = st.columns(3)
                        with pc1:
                            patch_voice = st.selectbox("ì„±ìš°", VOICE_OPTIONS, index=loaded_voice_idx, key="patch_voice")
                        with pc2:
                            patch_speed = st.slider("ì†ë„", 0.5, 2.0, loaded_speed, 0.1, key="patch_speed")
                        with pc3:
                            # [ì¶”ê°€ë¨] ì¬ë…¹ìŒìš© ë³¼ë¥¨
                            patch_vol = st.slider("ë³¼ë¥¨(dB)", -10.0, 10.0, 0.0, 1.0, key="patch_vol")
                            
                        patch_prompt = st.text_input("ìŠ¤íƒ€ì¼ ì§€ì‹œ", loaded_prompt, key="patch_prompt")

                    if st.button("ğŸ©¹ ì¬ë…¹ìŒ ë° ë®ì–´ì”Œìš°ê¸°", type="secondary", use_container_width=True):
                        if not patch_text.strip():
                            st.error("ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
                        else:
                            with st.spinner("ìƒì„± ì¤‘..."):
                                # 1. ì˜¤ë””ì˜¤ ìƒì„±
                                raw_audio = generate_speech(api_key, patch_text, patch_prompt, patch_voice, 1.0, patch_speed, patch_vol)
                                new_part = add_silence_padding(raw_audio, duration_sec=0.1)
                                
                                if new_part and os.path.exists(audio_full_path):
                                    # ìƒì„±ëœ ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì • (ì¤‘ìš”: ì •í™•í•œ ë³´ì •ì„ ìœ„í•´)
                                    import wave
                                    with wave.open(io.BytesIO(new_part), 'rb') as w:
                                        frames = w.getnframes()
                                        rate = w.getframerate()
                                        new_duration = frames / float(rate)

                                    with open(audio_full_path, "rb") as f: orig = f.read()
                                    patched = patch_audio_segment(orig, new_part, start_time, end_time)
                                    
                                    if patched:
                                        # 2. ì˜¤ë””ì˜¤ ì €ì¥
                                        with open(audio_full_path, "wb") as f: f.write(patched)
                                        
                                        # 3. [í•µì‹¬] íƒ€ì„ë¼ì¸ ë°ì´í„° ì¦‰ì‹œ ë³´ì •
                                        if verify_key in st.session_state:
                                            old_timeline = st.session_state[verify_key]
                                            # ìˆ˜í•™ì  ê³„ì‚°ìœ¼ë¡œ ì‹œê°„ ë°€ê¸°/ë‹¹ê¸°ê¸°
                                            new_timeline = adjust_timeline_for_patch(old_timeline, start_time, end_time, new_duration)
                                            st.session_state[verify_key] = new_timeline
                                            st.toast("âœ… ì¬ë…¹ìŒ ì™„ë£Œ! íƒ€ì„ë¼ì¸ ì‹±í¬ê°€ ìë™ ì¡°ì ˆë˜ì—ˆìŠµë‹ˆë‹¤.")
                                        else:
                                            st.success("ìˆ˜ì„  ì™„ë£Œ")

                                        time.sleep(0.5)
                                        st.rerun()

if __name__ == "__main__":
    main()